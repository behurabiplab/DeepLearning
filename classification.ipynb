{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy Formula\n",
    "\n",
    "The formula for entropy is:\n",
    "\n",
    "$$\n",
    "H(X) = - \\sum_{i=1}^{n} P(x_i) \\log_2 P(x_i)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( H(X) \\): Entropy of the random variable \\( X \\)\n",
    "- \\( P(x_i) \\): Probability of the \\( i^{th} \\) event\n",
    "- \\( n \\): Total number of possible events\n",
    "- \\( \\log_b \\): Logarithm with base \\( b \\) (commonly base 2 for information entropy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Using numpy\n",
    "p = [0.1,0.9]\n",
    "H = 0  \n",
    "for i in p:\n",
    "    H += -i * np.log(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy is 0.5623351446188083\n"
     ]
    }
   ],
   "source": [
    "p = [0.25, 0.75]\n",
    "H = 0\n",
    "for i in p:\n",
    "    H +=   -i * np.log(i)\n",
    "print(f\"Entropy is {H}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Entropy Formula\n",
    "\n",
    "The formula for cross-entropy is:\n",
    "\n",
    "$$\n",
    "H(p, q) = - \\sum_{i=1}^{n} p(x_i) \\log q(x_i)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( H(p, q) \\): Cross-entropy between the true distribution \\( p \\) and the predicted distribution \\( q \\)\n",
    "- \\( p(x_i) \\): True probability of the \\( i^{th} \\) class\n",
    "- \\( q(x_i) \\): Predicted probability of the \\( i^{th} \\) class\n",
    "- \\( n \\): Total number of classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corss Entropy 1.3862943611198906\n",
      "Cross Entropy 75.0\n",
      "Cross Entropy 1.3862943649291992\n"
     ]
    }
   ],
   "source": [
    "p =[1,0.0]\n",
    "q = [0.25, 0.75]\n",
    "H = 0\n",
    "for i,j in zip(p,q):\n",
    "    H += -i* np.log(j)\n",
    "print(f\"Corss Entropy {H}\")\n",
    "\n",
    "tensor_p = torch.tensor(p) \n",
    "tensor_q = torch.tensor(q)\n",
    "\n",
    "##Due to some reason the order impact the entopy as shown below. This is applied for torch tensor\n",
    "H = F.binary_cross_entropy(tensor_p, tensor_q)\n",
    "print(f\"Cross Entropy {H}\")\n",
    "H = F.binary_cross_entropy(tensor_q, tensor_p)\n",
    "print(f\"Cross Entropy {H}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "argmin returns the position of the minimum number\n",
    "\n",
    "argmax returns the position of the maximum number. The argmax functin applies on the output  probability to detect the postion. This position tell us the category.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
